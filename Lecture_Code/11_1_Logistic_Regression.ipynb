{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/drbob-richardson/stat220/blob/main/Lecture_Code/11_1_Logistic_Regression.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"v6Fi9LjX1vKO"},"source":["Load libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1855,"status":"ok","timestamp":1701937071299,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"C9SZr5Pqj57N"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"markdown","metadata":{"id":"tmbXA9Gw1w86"},"source":["The raisin data set can be used to build a model to classify raisins as either Kecimen or Besni."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":1042,"status":"ok","timestamp":1701937088845,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"RuqzRsWWgKf6","outputId":"1b4b6301-ae14-42a9-d7a8-d103afcb1702"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-d969e42e-3ec2-460a-863a-9833d075df8d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Area</th>\n","      <th>MajorAxisLength</th>\n","      <th>MinorAxisLength</th>\n","      <th>Eccentricity</th>\n","      <th>ConvexArea</th>\n","      <th>Extent</th>\n","      <th>Perimeter</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>87524</td>\n","      <td>442.246011</td>\n","      <td>253.291155</td>\n","      <td>0.819738</td>\n","      <td>90546</td>\n","      <td>0.758651</td>\n","      <td>1184.040</td>\n","      <td>Kecimen</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>75166</td>\n","      <td>406.690687</td>\n","      <td>243.032436</td>\n","      <td>0.801805</td>\n","      <td>78789</td>\n","      <td>0.684130</td>\n","      <td>1121.786</td>\n","      <td>Kecimen</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>90856</td>\n","      <td>442.267048</td>\n","      <td>266.328318</td>\n","      <td>0.798354</td>\n","      <td>93717</td>\n","      <td>0.637613</td>\n","      <td>1208.575</td>\n","      <td>Kecimen</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>45928</td>\n","      <td>286.540559</td>\n","      <td>208.760042</td>\n","      <td>0.684989</td>\n","      <td>47336</td>\n","      <td>0.699599</td>\n","      <td>844.162</td>\n","      <td>Kecimen</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>79408</td>\n","      <td>352.190770</td>\n","      <td>290.827533</td>\n","      <td>0.564011</td>\n","      <td>81463</td>\n","      <td>0.792772</td>\n","      <td>1073.251</td>\n","      <td>Kecimen</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>895</th>\n","      <td>83248</td>\n","      <td>430.077308</td>\n","      <td>247.838695</td>\n","      <td>0.817263</td>\n","      <td>85839</td>\n","      <td>0.668793</td>\n","      <td>1129.072</td>\n","      <td>Besni</td>\n","    </tr>\n","    <tr>\n","      <th>896</th>\n","      <td>87350</td>\n","      <td>440.735698</td>\n","      <td>259.293149</td>\n","      <td>0.808629</td>\n","      <td>90899</td>\n","      <td>0.636476</td>\n","      <td>1214.252</td>\n","      <td>Besni</td>\n","    </tr>\n","    <tr>\n","      <th>897</th>\n","      <td>99657</td>\n","      <td>431.706981</td>\n","      <td>298.837323</td>\n","      <td>0.721684</td>\n","      <td>106264</td>\n","      <td>0.741099</td>\n","      <td>1292.828</td>\n","      <td>Besni</td>\n","    </tr>\n","    <tr>\n","      <th>898</th>\n","      <td>93523</td>\n","      <td>476.344094</td>\n","      <td>254.176054</td>\n","      <td>0.845739</td>\n","      <td>97653</td>\n","      <td>0.658798</td>\n","      <td>1258.548</td>\n","      <td>Besni</td>\n","    </tr>\n","    <tr>\n","      <th>899</th>\n","      <td>85609</td>\n","      <td>512.081774</td>\n","      <td>215.271976</td>\n","      <td>0.907345</td>\n","      <td>89197</td>\n","      <td>0.632020</td>\n","      <td>1272.862</td>\n","      <td>Besni</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>900 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d969e42e-3ec2-460a-863a-9833d075df8d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d969e42e-3ec2-460a-863a-9833d075df8d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d969e42e-3ec2-460a-863a-9833d075df8d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-41b20e5a-98bb-4972-9274-a127229de97d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-41b20e5a-98bb-4972-9274-a127229de97d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-41b20e5a-98bb-4972-9274-a127229de97d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["      Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n","0    87524       442.246011       253.291155      0.819738       90546   \n","1    75166       406.690687       243.032436      0.801805       78789   \n","2    90856       442.267048       266.328318      0.798354       93717   \n","3    45928       286.540559       208.760042      0.684989       47336   \n","4    79408       352.190770       290.827533      0.564011       81463   \n","..     ...              ...              ...           ...         ...   \n","895  83248       430.077308       247.838695      0.817263       85839   \n","896  87350       440.735698       259.293149      0.808629       90899   \n","897  99657       431.706981       298.837323      0.721684      106264   \n","898  93523       476.344094       254.176054      0.845739       97653   \n","899  85609       512.081774       215.271976      0.907345       89197   \n","\n","       Extent  Perimeter    Class  \n","0    0.758651   1184.040  Kecimen  \n","1    0.684130   1121.786  Kecimen  \n","2    0.637613   1208.575  Kecimen  \n","3    0.699599    844.162  Kecimen  \n","4    0.792772   1073.251  Kecimen  \n","..        ...        ...      ...  \n","895  0.668793   1129.072    Besni  \n","896  0.636476   1214.252    Besni  \n","897  0.741099   1292.828    Besni  \n","898  0.658798   1258.548    Besni  \n","899  0.632020   1272.862    Besni  \n","\n","[900 rows x 8 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["raisin = pd.read_csv(\"https://richardson.byu.edu/220/raisin.csv\")\n","raisin"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"elapsed":453,"status":"ok","timestamp":1701937199566,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"ylbhkU-nn5PB","outputId":"8ece426c-4a68-40b0-befa-3d00001ac8bd"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"],"text/plain":["LogisticRegression()"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, confusion_matrix, auc\n","\n","# The y variable is the target. It must be either 0's and 1's\n","# or True's and False's. We can easily turn a variable with\n","# strings in to a True/False\n","y = raisin.Class == \"Kecimen\"\n","\n","# Set of predictors\n","X = raisin.drop(columns = [\"Class\"])\n","\n","# The logistic regression module cn be used to build\n","# a logistic regression model\n","mod = LogisticRegression()\n","mod.fit(X,y)\n"]},{"cell_type":"markdown","metadata":{"id":"SE0ofeq02O0y"},"source":["The fitting procedure is still gradient descent but target function is a little more volatile than anything we've seen before. We can play around with different optimizers and different optimization settings, but instead let's just standardize the data, which always makes numerical algorithms work better."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701937297899,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"yELGxPf6pN6_","outputId":"148b67e9-22b5-4e83-d52a-60ede48a965e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n","  warnings.warn(\n"]},{"data":{"text/plain":["array([[-19.52470455,   5.17099581,   4.55080239,   0.35104722,\n","         16.65223921,   0.03647395,  -9.88413897]])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Scale the data\n","scale_for_X = StandardScaler()\n","scale_for_X.fit(X)\n","scaled_X = scale_for_X.transform(X)\n","scaled_X = pd.DataFrame(scaled_X,columns = X.columns)\n","\n","# Fit a model. We will use the option penalty = \"none\", I'll explain that later.\n","mod = LogisticRegression(penalty = \"none\")\n","mod.fit(scaled_X,y)\n","# print the coefficients\n","mod.coef_"]},{"cell_type":"markdown","metadata":{"id":"zSvfrFt9209a"},"source":["Predict the data."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":159,"status":"ok","timestamp":1701937309254,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"KS7I7LIurnUC","outputId":"718d0ba7-181d-4ae8-ee62-05e958bb34d5"},"outputs":[{"data":{"text/plain":["array([False,  True, False,  True,  True,  True,  True,  True,  True,\n","        True, False,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True, False,  True,  True,\n","        True,  True,  True, False,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True, False,  True,  True,  True, False,\n","        True,  True,  True,  True,  True,  True, False,  True,  True,\n","       False,  True,  True,  True,  True,  True,  True,  True, False,\n","        True,  True,  True,  True, False,  True,  True, False, False,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True, False,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True, False,  True,  True,  True, False, False, False,\n","        True,  True,  True,  True,  True,  True,  True, False,  True,\n","        True,  True,  True, False,  True,  True,  True,  True,  True,\n","        True, False,  True,  True, False,  True,  True, False,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True, False,  True,  True,  True,  True,  True,  True,\n","        True, False,  True,  True,  True,  True, False,  True, False,\n","        True,  True, False,  True,  True,  True,  True,  True,  True,\n","       False,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True, False,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True, False, False,  True,  True,  True,  True, False,  True,\n","        True, False,  True,  True,  True,  True,  True, False,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True, False, False,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True, False,  True,  True, False,  True,  True,  True,\n","        True, False,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True, False, False,  True,  True, False,\n","        True,  True,  True,  True,  True,  True, False, False,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True, False,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True, False, False,  True,\n","        True,  True, False,  True,  True,  True,  True, False,  True,\n","        True,  True,  True,  True, False,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True, False, False,  True,  True,\n","        True,  True, False,  True,  True, False, False, False,  True,\n","       False, False, False, False,  True, False, False, False, False,\n","       False, False, False, False,  True, False,  True, False, False,\n","       False, False,  True, False, False, False, False, False, False,\n","       False, False, False, False,  True, False,  True, False, False,\n","       False, False, False, False, False, False, False,  True, False,\n","       False, False, False, False, False, False, False,  True, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False,  True, False, False, False, False, False, False,\n","       False, False, False, False,  True, False,  True, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","        True, False, False, False, False, False, False, False, False,\n","       False, False, False, False,  True, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","        True, False, False, False, False, False, False, False, False,\n","       False, False,  True, False,  True, False, False, False, False,\n","       False, False, False,  True,  True, False, False,  True,  True,\n","       False, False,  True, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False,  True, False,  True, False, False, False,\n","        True, False, False,  True, False, False, False, False,  True,\n","        True, False, False, False, False, False, False,  True, False,\n","       False, False, False,  True, False,  True, False, False,  True,\n","       False, False, False, False, False, False, False, False, False,\n","       False,  True, False, False, False,  True, False, False,  True,\n","       False, False, False, False,  True, False, False, False, False,\n","        True, False, False, False,  True, False, False, False, False,\n","       False, False, False, False,  True, False, False, False,  True,\n","       False, False, False, False, False, False,  True, False, False,\n","        True, False, False, False, False, False, False, False,  True,\n","       False, False,  True, False, False, False, False, False,  True,\n","       False, False, False, False, False, False,  True, False, False,\n","       False, False, False, False, False,  True, False, False, False,\n","       False, False, False, False, False, False, False, False,  True,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False,  True,\n","       False, False,  True, False, False, False, False, False, False,\n","       False, False, False, False, False, False,  True, False, False,\n","       False, False, False, False, False, False,  True, False, False,\n","       False, False, False, False, False, False,  True,  True, False,\n","       False, False, False,  True, False, False,  True,  True,  True,\n","        True, False, False,  True, False, False, False, False, False,\n","       False,  True,  True,  True, False, False, False, False, False,\n","        True, False,  True, False, False,  True,  True, False, False,\n","       False, False, False, False, False, False,  True, False, False,\n","       False, False, False, False, False, False, False, False,  True,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False,  True, False, False,  True, False, False,\n","       False, False, False, False,  True, False, False, False, False])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["mod.predict(scaled_X)"]},{"cell_type":"markdown","metadata":{"id":"pLbsHnHV3UMt"},"source":["The score is just a measure of accuracy."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":172,"status":"ok","timestamp":1701937366005,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"at1O0oX73TgC","outputId":"39e3306d-9671-4b5d-ce7e-edb059c50ce2"},"outputs":[{"data":{"text/plain":["0.8577777777777778"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["mod.score(scaled_X, y)"]},{"cell_type":"markdown","metadata":{"id":"LxPiwQk524Pk"},"source":["The logistic regression predictions are in reality more than just a single True.False or 1/0. They actually give a probability, which in this case is a probability of the raisin being Kecimen."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203,"status":"ok","timestamp":1701937372978,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"fSspOT6_rr3S","outputId":"119bee18-8224-4719-d699-21d33f5d7c5f"},"outputs":[{"data":{"text/plain":["array([[0.70717711, 0.29282289],\n","       [0.47205672, 0.52794328],\n","       [0.75368935, 0.24631065],\n","       [0.04702848, 0.95297152],\n","       [0.12942462, 0.87057538],\n","       [0.06989084, 0.93010916],\n","       [0.06552195, 0.93447805],\n","       [0.04848009, 0.95151991],\n","       [0.07251451, 0.92748549],\n","       [0.10328657, 0.89671343],\n","       [0.7397053 , 0.2602947 ],\n","       [0.04162184, 0.95837816],\n","       [0.02346831, 0.97653169],\n","       [0.07737765, 0.92262235],\n","       [0.35246604, 0.64753396],\n","       [0.03593172, 0.96406828],\n","       [0.38526903, 0.61473097],\n","       [0.09550701, 0.90449299],\n","       [0.03903862, 0.96096138],\n","       [0.43171944, 0.56828056]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["mod.predict_proba(scaled_X)[0:20]"]},{"cell_type":"markdown","metadata":{"id":"CYyRk0E23YHz"},"source":["The AUC is a metric that accounts for these probabilities. Accuracy only cares if it is right or not. AUC will include how confident it was that it was right. In other words, if a raisin is Kecimen, a model that predicts a raisin is Kecimen with probability 0.9 will score higher than a model that predicts Kecimen with a probability of 0.6, even though in accuracy they would both score the same."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1701937450416,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"xgF8ySdasLOC","outputId":"58d5d541-7300-42ac-b30a-25bb0b742def"},"outputs":[{"data":{"text/plain":["0.9279061728395063"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import roc_auc_score\n","roc_auc_score(y,mod.predict_proba(scaled_X)[:,1])"]},{"cell_type":"markdown","metadata":{"id":"udfpNaDe4RT1"},"source":["A confusion matrix can show us how many false negatives and false positives we have. It can be helpful in cases where there is a large unbalance.\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190,"status":"ok","timestamp":1701937500073,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"w5K3eDJcsiUq","outputId":"0152a600-cfaa-40d8-c8f3-adb6998d8408"},"outputs":[{"data":{"text/plain":["array([[379,  71],\n","       [ 57, 393]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(y, mod.predict(scaled_X))"]},{"cell_type":"markdown","metadata":{"id":"MfH5_wUXRT1T"},"source":["The default cutoff is 0.5, but in many cases, especially where the number of 1's and 0's is highly unbalanced, yoou might want to choose a different cutoff."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1701937625252,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"YMdP3o_0ZSsO","outputId":"bd531e4e-1f17-411a-a81a-68e21908ac48"},"outputs":[{"data":{"text/plain":["array([[379,  71],\n","       [ 57, 393]])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(y,mod.predict_proba(scaled_X)[:,1] > 0.5)"]},{"cell_type":"markdown","metadata":{"id":"IwJsc1Uk_7lI"},"source":["We can use the statsmodels package to do things like find significance of variables."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1205,"status":"ok","timestamp":1701937646773,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"l-feUjCKsnYy","outputId":"f60dce29-00cb-4755-87cc-338bc25743bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimization terminated successfully.\n","         Current function value: 0.338323\n","         Iterations 9\n"]}],"source":["import statsmodels.api as sm\n","scaled_X1 = sm.add_constant(scaled_X)\n","mod2 = sm.Logit(y, scaled_X1)\n","result = mod2.fit()"]},{"cell_type":"markdown","metadata":{"id":"4TtQq8FCACZG"},"source":["We can get the p-values for the parameters and remove insignificant variables."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1701937649743,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"N4yppQb4vBws","outputId":"17feae03-cf50-4759-e126-54047a9f6017"},"outputs":[{"data":{"text/html":["<table class=\"simpletable\">\n","<caption>Logit Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>   <td>   900</td>  \n","</tr>\n","<tr>\n","  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>   892</td>  \n","</tr>\n","<tr>\n","  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     7</td>  \n","</tr>\n","<tr>\n","  <th>Date:</th>            <td>Thu, 07 Dec 2023</td> <th>  Pseudo R-squ.:     </th>   <td>0.5119</td>  \n","</tr>\n","<tr>\n","  <th>Time:</th>                <td>08:27:29</td>     <th>  Log-Likelihood:    </th>  <td> -304.49</td> \n","</tr>\n","<tr>\n","  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -623.83</td> \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.133e-133</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","         <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th>           <td>   -0.6897</td> <td>    0.230</td> <td>   -3.004</td> <td> 0.003</td> <td>   -1.140</td> <td>   -0.240</td>\n","</tr>\n","<tr>\n","  <th>Area</th>            <td>  -19.5311</td> <td>    4.851</td> <td>   -4.026</td> <td> 0.000</td> <td>  -29.039</td> <td>  -10.023</td>\n","</tr>\n","<tr>\n","  <th>MajorAxisLength</th> <td>    5.1708</td> <td>    1.852</td> <td>    2.792</td> <td> 0.005</td> <td>    1.541</td> <td>    8.801</td>\n","</tr>\n","<tr>\n","  <th>MinorAxisLength</th> <td>    4.5508</td> <td>    1.346</td> <td>    3.381</td> <td> 0.001</td> <td>    1.913</td> <td>    7.189</td>\n","</tr>\n","<tr>\n","  <th>Eccentricity</th>    <td>    0.3512</td> <td>    0.443</td> <td>    0.792</td> <td> 0.428</td> <td>   -0.518</td> <td>    1.220</td>\n","</tr>\n","<tr>\n","  <th>ConvexArea</th>      <td>   16.6605</td> <td>    4.852</td> <td>    3.434</td> <td> 0.001</td> <td>    7.152</td> <td>   26.169</td>\n","</tr>\n","<tr>\n","  <th>Extent</th>          <td>    0.0365</td> <td>    0.145</td> <td>    0.251</td> <td> 0.802</td> <td>   -0.248</td> <td>    0.321</td>\n","</tr>\n","<tr>\n","  <th>Perimeter</th>       <td>   -9.8858</td> <td>    1.810</td> <td>   -5.463</td> <td> 0.000</td> <td>  -13.433</td> <td>   -6.339</td>\n","</tr>\n","</table>"],"text/latex":["\\begin{center}\n","\\begin{tabular}{lclc}\n","\\toprule\n","\\textbf{Dep. Variable:}   &      Class       & \\textbf{  No. Observations:  } &      900    \\\\\n","\\textbf{Model:}           &      Logit       & \\textbf{  Df Residuals:      } &      892    \\\\\n","\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        7    \\\\\n","\\textbf{Date:}            & Thu, 07 Dec 2023 & \\textbf{  Pseudo R-squ.:     } &   0.5119    \\\\\n","\\textbf{Time:}            &     08:27:29     & \\textbf{  Log-Likelihood:    } &   -304.49   \\\\\n","\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -623.83   \\\\\n","\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } & 1.133e-133  \\\\\n","\\bottomrule\n","\\end{tabular}\n","\\begin{tabular}{lcccccc}\n","                         & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n","\\midrule\n","\\textbf{const}           &      -0.6897  &        0.230     &    -3.004  &         0.003        &       -1.140    &       -0.240     \\\\\n","\\textbf{Area}            &     -19.5311  &        4.851     &    -4.026  &         0.000        &      -29.039    &      -10.023     \\\\\n","\\textbf{MajorAxisLength} &       5.1708  &        1.852     &     2.792  &         0.005        &        1.541    &        8.801     \\\\\n","\\textbf{MinorAxisLength} &       4.5508  &        1.346     &     3.381  &         0.001        &        1.913    &        7.189     \\\\\n","\\textbf{Eccentricity}    &       0.3512  &        0.443     &     0.792  &         0.428        &       -0.518    &        1.220     \\\\\n","\\textbf{ConvexArea}      &      16.6605  &        4.852     &     3.434  &         0.001        &        7.152    &       26.169     \\\\\n","\\textbf{Extent}          &       0.0365  &        0.145     &     0.251  &         0.802        &       -0.248    &        0.321     \\\\\n","\\textbf{Perimeter}       &      -9.8858  &        1.810     &    -5.463  &         0.000        &      -13.433    &       -6.339     \\\\\n","\\bottomrule\n","\\end{tabular}\n","%\\caption{Logit Regression Results}\n","\\end{center}"],"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                           Logit Regression Results                           \n","==============================================================================\n","Dep. Variable:                  Class   No. Observations:                  900\n","Model:                          Logit   Df Residuals:                      892\n","Method:                           MLE   Df Model:                            7\n","Date:                Thu, 07 Dec 2023   Pseudo R-squ.:                  0.5119\n","Time:                        08:27:29   Log-Likelihood:                -304.49\n","converged:                       True   LL-Null:                       -623.83\n","Covariance Type:            nonrobust   LLR p-value:                1.133e-133\n","===================================================================================\n","                      coef    std err          z      P>|z|      [0.025      0.975]\n","-----------------------------------------------------------------------------------\n","const              -0.6897      0.230     -3.004      0.003      -1.140      -0.240\n","Area              -19.5311      4.851     -4.026      0.000     -29.039     -10.023\n","MajorAxisLength     5.1708      1.852      2.792      0.005       1.541       8.801\n","MinorAxisLength     4.5508      1.346      3.381      0.001       1.913       7.189\n","Eccentricity        0.3512      0.443      0.792      0.428      -0.518       1.220\n","ConvexArea         16.6605      4.852      3.434      0.001       7.152      26.169\n","Extent              0.0365      0.145      0.251      0.802      -0.248       0.321\n","Perimeter          -9.8858      1.810     -5.463      0.000     -13.433      -6.339\n","===================================================================================\n","\"\"\""]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["result.summary()"]},{"cell_type":"markdown","metadata":{"id":"H-kcSlMdANrm"},"source":["Let's remove the penalty = \"none\" variable. This actually does regularization, a.k.a. Lasso type shrinkage automatically. This will ofte result in a better model."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1701937697105,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"Dsd37K5xzzEn","outputId":"928d7718-9374-4ba2-c0e6-0e32654a035f"},"outputs":[{"data":{"text/plain":["0.8666666666666667"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["mod = LogisticRegression()\n","mod.fit(scaled_X,y)\n","mod.score(scaled_X, y)"]},{"cell_type":"markdown","metadata":{"id":"jM4O8CugAa__"},"source":["You can adjust the penalization term with C = .... The default is C = 1."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1701937733320,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"VInLi0uIz94g","outputId":"1b051da8-7b3a-4eee-95cd-2fbbca8be911"},"outputs":[{"data":{"text/plain":["0.8688888888888889"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["mod = LogisticRegression(C = 5)\n","mod.fit(scaled_X,y)\n","mod.score(scaled_X, y)"]},{"cell_type":"markdown","metadata":{"id":"nwwrbuMPAfUn"},"source":["Best to compare on a holdout set to find a good value for C."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":209,"status":"ok","timestamp":1701937741800,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"sr2bIzNp1BSn"},"outputs":[],"source":["scaled_X_train, scaled_X_test, y_train, y_test = train_test_split(scaled_X,y,test_size=0.3, random_state=1357)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701937810732,"user":{"displayName":"Robert Richardson","userId":"16108946315450400305"},"user_tz":420},"id":"BGrakfub1n46","outputId":"d3dced4d-f123-4a9c-d8cd-2ebcab11b858"},"outputs":[{"data":{"text/plain":["0.8592592592592593"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["mod = LogisticRegression(penalty = None)\n","mod.fit(scaled_X_train,y_train)\n","mod.score(scaled_X_test, y_test)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOaqfUGknqteEGu+XB6wYLP","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
